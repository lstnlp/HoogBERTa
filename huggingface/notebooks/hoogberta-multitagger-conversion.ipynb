{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install Depedencies","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers subword-nmt attacut tokenizers numpy==1.21.6 gdown torchmetrics==0.7","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:50:15.563247Z","iopub.execute_input":"2023-04-05T18:50:15.563592Z","iopub.status.idle":"2023-04-05T18:50:31.651903Z","shell.execute_reply.started":"2023-04-05T18:50:15.563560Z","shell.execute_reply":"2023-04-05T18:50:31.651014Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/lstnlp/hoogberta\n%cd hoogberta\n!pip install -q torchtext==0.6.0 fairseq==0.10.2 pytorch-lightning==1.4.7 textsearch seqeval\n!pip install -q --no-dependencies --editable  .","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:50:31.653373Z","iopub.execute_input":"2023-04-05T18:50:31.653694Z","iopub.status.idle":"2023-04-05T18:50:58.682573Z","shell.execute_reply.started":"2023-04-05T18:50:31.653662Z","shell.execute_reply":"2023-04-05T18:50:58.681678Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'hoogberta'...\nremote: Enumerating objects: 267, done.\u001b[K\nremote: Counting objects: 100% (106/106), done.\u001b[K\nremote: Compressing objects: 100% (63/63), done.\u001b[K\nremote: Total 267 (delta 63), reused 73 (delta 43), pack-reused 161\u001b[K\nReceiving objects: 100% (267/267), 4.05 MiB | 21.81 MiB/s, done.\nResolving deltas: 100% (136/136), done.\n/kaggle/working/hoogberta\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from hoogberta import download\ndownload()","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:50:58.683562Z","iopub.execute_input":"2023-04-05T18:50:58.683855Z","iopub.status.idle":"2023-04-05T18:51:13.951623Z","shell.execute_reply.started":"2023-04-05T18:50:58.683827Z","shell.execute_reply":"2023-04-05T18:51:13.950706Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1xQHDAE8nbFu2wAM6SAXtTjk890JWLUhy\nTo: /kaggle/working/hoogberta/dict.zip\n100%|██████████| 1.15k/1.15k [00:00<00:00, 2.54MB/s]\nDownloading...\nFrom (uriginal): https://drive.google.com/uc?id=1bBSWQzzEt99mYd_EY5W-lQKW6L-D8axW\nFrom (redirected): https://drive.google.com/uc?id=1bBSWQzzEt99mYd_EY5W-lQKW6L-D8axW&confirm=t&uuid=d52406ec-36f8-4bb2-a592-bff702324407\nTo: /kaggle/working/hoogberta/modelL12.pt\n100%|██████████| 575M/575M [00:02<00:00, 265MB/s]  \nDownloading...\nFrom (uriginal): https://drive.google.com/uc?id=1fYtRAyh6d4W9LVCSJiSYKKM_CCPflBc9\nFrom (redirected): https://drive.google.com/uc?id=1fYtRAyh6d4W9LVCSJiSYKKM_CCPflBc9&confirm=t&uuid=91a0483e-de82-41b9-a03e-6782ff233066\nTo: /kaggle/working/hoogberta/checkpoint_best.pt\n100%|██████████| 1.44G/1.44G [00:06<00:00, 207MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1ZNxpVHNZbAfdWA-wu7iMSUtcySzQCJam\nTo: /kaggle/working/hoogberta/dict.txt\n100%|██████████| 1.09M/1.09M [00:00<00:00, 125MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1ct9xaAUkqxbn9X8JgO8yKfBYWT1Dthld\nTo: /kaggle/working/hoogberta/th_18M.50000.bpe\n100%|██████████| 818k/818k [00:00<00:00, 127MB/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Convert Model","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/hoogberta","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:51:13.953526Z","iopub.execute_input":"2023-04-05T18:51:13.953991Z","iopub.status.idle":"2023-04-05T18:51:13.959697Z","shell.execute_reply.started":"2023-04-05T18:51:13.953960Z","shell.execute_reply":"2023-04-05T18:51:13.958522Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/working/hoogberta\n","output_type":"stream"}]},{"cell_type":"code","source":"base_path = '/kaggle/working/hoogberta'","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:51:13.961150Z","iopub.execute_input":"2023-04-05T18:51:13.961564Z","iopub.status.idle":"2023-04-05T18:51:14.120217Z","shell.execute_reply.started":"2023-04-05T18:51:13.961525Z","shell.execute_reply":"2023-04-05T18:51:14.118723Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from hoogberta.multitagger import HoogBERTaMuliTaskTagger\ntagger = HoogBERTaMuliTaskTagger(cuda=False) # or cuda=True\noutput = tagger.nlp(\"วันที่ 12 มีนาคมนี้ ฉันจะไปเที่ยววัดพระแก้ว ที่กรุงเทพ\")","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:51:14.121567Z","iopub.execute_input":"2023-04-05T18:51:14.121928Z","iopub.status.idle":"2023-04-05T18:51:28.119055Z","shell.execute_reply.started":"2023-04-05T18:51:14.121895Z","shell.execute_reply":"2023-04-05T18:51:28.117910Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch\nhoogberta_base_checkpoint = torch.load('/kaggle/working/hoogberta/models/hoogberta_base/checkpoint_best.pt', map_location=torch.device('cpu'))","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:51:28.120604Z","iopub.execute_input":"2023-04-05T18:51:28.121007Z","iopub.status.idle":"2023-04-05T18:51:28.635070Z","shell.execute_reply.started":"2023-04-05T18:51:28.120964Z","shell.execute_reply":"2023-04-05T18:51:28.634079Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"hoogberta_base_checkpoint['model'] = tagger.model.encoder.model.state_dict()\ntorch.save(hoogberta_base_checkpoint, '/kaggle/working/hoogberta/models/hoogberta_base/model.pt')","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:51:28.636312Z","iopub.execute_input":"2023-04-05T18:51:28.636660Z","iopub.status.idle":"2023-04-05T18:51:30.308647Z","shell.execute_reply.started":"2023-04-05T18:51:28.636627Z","shell.execute_reply":"2023-04-05T18:51:30.307700Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from transformers.models.roberta.convert_roberta_original_pytorch_checkpoint_to_pytorch import convert_roberta_checkpoint_to_pytorch\n\nconvert_roberta_checkpoint_to_pytorch('models/hoogberta_base', 'converted_model_l12', False)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:51:30.310028Z","iopub.execute_input":"2023-04-05T18:51:30.310397Z","iopub.status.idle":"2023-04-05T18:51:38.257330Z","shell.execute_reply.started":"2023-04-05T18:51:30.310361Z","shell.execute_reply":"2023-04-05T18:51:38.256288Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"1042301B [00:00, 49378519.83B/s]\n456318B [00:00, 35098778.89B/s]\n","output_type":"stream"},{"name":"stdout","text":"Our BERT config: RobertaConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.27.4\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 74905\n}\n\n","output_type":"stream"},{"name":"stderr","text":"Configuration saved in converted_model_l12/config.json\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 11, 74905]) torch.Size([1, 11, 74905])\nmax_absolute_diff = 1.9073486328125e-06\nDo both models output the same tensors? 🔥\nSaving model to converted_model_l12\n","output_type":"stream"},{"name":"stderr","text":"Model weights saved in converted_model_l12/pytorch_model.bin\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, RobertaForTokenClassification, RobertaModel\nfrom attacut import tokenize\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"new5558/HoogBERTa\")\n\nhuggingface_model_classification = RobertaForTokenClassification.from_pretrained('converted_model_l12')","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:51:38.261220Z","iopub.execute_input":"2023-04-05T18:51:38.261983Z","iopub.status.idle":"2023-04-05T18:51:41.761650Z","shell.execute_reply.started":"2023-04-05T18:51:38.261947Z","shell.execute_reply":"2023-04-05T18:51:41.759994Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/288 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49f7ddde5c954701847f4d759e1d6cac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d59555a29cbd485fba6357e27b524c5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/120 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4d18caa076d4d25abdaf6d4a6132e22"}},"metadata":{}},{"name":"stderr","text":"loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--new5558--HoogBERTa/snapshots/0fadd23185e3ba91012653a71962c280fe97fa67/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--new5558--HoogBERTa/snapshots/0fadd23185e3ba91012653a71962c280fe97fa67/special_tokens_map.json\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--new5558--HoogBERTa/snapshots/0fadd23185e3ba91012653a71962c280fe97fa67/tokenizer_config.json\nloading configuration file converted_model_l12/config.json\nModel config RobertaConfig {\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.27.4\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 74905\n}\n\nloading weights file converted_model_l12/pytorch_model.bin\nSome weights of the model checkpoint at converted_model_l12 were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForTokenClassification were not initialized from the model checkpoint at converted_model_l12 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"huggingface_model_classification.classifier = tagger.model.fc_ne","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:51:41.762986Z","iopub.execute_input":"2023-04-05T18:51:41.763263Z","iopub.status.idle":"2023-04-05T18:51:41.770331Z","shell.execute_reply.started":"2023-04-05T18:51:41.763236Z","shell.execute_reply":"2023-04-05T18:51:41.768775Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"sentence = \"วันที่ 12 มีนาคมนี้ ฉันจะไปเที่ยววัดพระแก้ว ที่กรุงเทพ\"\n\nhuggingface_model_classification.classifier = tagger.model.fc_ne.eval()\ntagger.model.eval()\nall_sent = []\nsentences = sentence.split(\" \")\nfor sent in sentences:\n    all_sent.append(\" \".join(tokenize(sent)).replace(\"_\",\"[!und:]\"))\n\nsentence = \" _ \".join(all_sent)\ntokenized_text = tokenizer(sentence, return_tensors = 'pt')\ntoken_ids = tokenized_text['input_ids']\n\nwith torch.no_grad():\n    ne_pred = huggingface_model_classification(**tokenized_text).logits\n    ne_out  =  ne_pred.argmax(dim = -1).view(-1).tolist()\n    ne   = [tagger.ne_dict[id] for id in ne_out]","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:51:41.772053Z","iopub.execute_input":"2023-04-05T18:51:41.772397Z","iopub.status.idle":"2023-04-05T18:51:41.975837Z","shell.execute_reply.started":"2023-04-05T18:51:41.772362Z","shell.execute_reply":"2023-04-05T18:51:41.974783Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"ne","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:51:41.977569Z","iopub.execute_input":"2023-04-05T18:51:41.978301Z","iopub.status.idle":"2023-04-05T18:51:41.986004Z","shell.execute_reply.started":"2023-04-05T18:51:41.978262Z","shell.execute_reply":"2023-04-05T18:51:41.984577Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"['<s>',\n 'B_DTM',\n 'I_DTM',\n 'I_DTM',\n 'I_DTM',\n 'I_DTM',\n 'E_DTM',\n 'E_DTM',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'B_LOC',\n 'O',\n 'O',\n 'B_LOC',\n '</s>']"},"metadata":{}}]},{"cell_type":"code","source":"output","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:51:41.988323Z","iopub.execute_input":"2023-04-05T18:51:41.989281Z","iopub.status.idle":"2023-04-05T18:51:42.004483Z","shell.execute_reply.started":"2023-04-05T18:51:41.989047Z","shell.execute_reply":"2023-04-05T18:51:42.003158Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[['วัน', 'NN', 'B_DTM', 'O'],\n ['ที่', 'PS', 'I_DTM', 'O'],\n [' ', 'PU', 'I_DTM', 'PUNC'],\n ['12', 'NU', 'I_DTM', 'O'],\n [' ', 'PU', 'I_DTM', 'PUNC'],\n ['มีนาคม', 'NN', 'E_DTM', 'O'],\n ['นี้', 'AJ', 'E_DTM', 'O'],\n [' ', 'PU', 'O', 'MARK'],\n ['ฉัน', 'PR', 'O', 'O'],\n ['จะ', 'AX', 'O', 'O'],\n ['ไป', 'AV', 'O', 'O'],\n ['เที่ยว', 'VV', 'O', 'O'],\n ['วัดพระแก้ว', 'NN', 'B_LOC', 'O'],\n [' ', 'PU', 'O', 'PUNC'],\n ['ที่', 'PS', 'O', 'O'],\n ['กรุงเทพ', 'NN', 'B_LOC', 'O']]"},"metadata":{}}]},{"cell_type":"code","source":"id2label = {}\nlabel2id = {}\nfor i in range(len(tagger.ne_dict.symbols)):\n    id2label[i] = tagger.ne_dict.symbols[i]\n    label2id[tagger.ne_dict.symbols[i]] = i","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:51:42.006162Z","iopub.execute_input":"2023-04-05T18:51:42.006658Z","iopub.status.idle":"2023-04-05T18:51:42.013957Z","shell.execute_reply.started":"2023-04-05T18:51:42.006624Z","shell.execute_reply":"2023-04-05T18:51:42.012857Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"huggingface_model_classification.config.id2label = id2label\nhuggingface_model_classification.config.label2id = label2id","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:51:42.015099Z","iopub.execute_input":"2023-04-05T18:51:42.015404Z","iopub.status.idle":"2023-04-05T18:51:42.026247Z","shell.execute_reply.started":"2023-04-05T18:51:42.015376Z","shell.execute_reply":"2023-04-05T18:51:42.025322Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\nsentence = \"วันที่ 12 มีนาคมนี้ ฉันจะไปเที่ยววัดพระแก้ว ที่กรุงเทพ\"\n\nall_sent = []\nsentences = sentence.split(\" \")\nfor sent in sentences:\n    all_sent.append(\" \".join(tokenize(sent)).replace(\"_\",\"[!und:]\"))\n\nsentence = \" _ \".join(all_sent)\n\nnlp = pipeline('token-classification', model=huggingface_model_classification, tokenizer=tokenizer, aggregation_strategy=\"none\")","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:51:42.027091Z","iopub.execute_input":"2023-04-05T18:51:42.027363Z","iopub.status.idle":"2023-04-05T18:51:53.241736Z","shell.execute_reply.started":"2023-04-05T18:51:42.027328Z","shell.execute_reply":"2023-04-05T18:51:53.240261Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"nlp(sentence)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:51:53.244831Z","iopub.execute_input":"2023-04-05T18:51:53.246314Z","iopub.status.idle":"2023-04-05T18:51:53.436760Z","shell.execute_reply.started":"2023-04-05T18:51:53.246263Z","shell.execute_reply":"2023-04-05T18:51:53.435595Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[{'entity': 'B_DTM',\n  'score': 0.98959374,\n  'index': 1,\n  'word': 'วัน</w>',\n  'start': 0,\n  'end': 3},\n {'entity': 'I_DTM',\n  'score': 0.9782606,\n  'index': 2,\n  'word': 'ที่</w>',\n  'start': 4,\n  'end': 7},\n {'entity': 'I_DTM',\n  'score': 0.9975866,\n  'index': 3,\n  'word': '_</w>',\n  'start': 8,\n  'end': 9},\n {'entity': 'I_DTM',\n  'score': 0.9989825,\n  'index': 4,\n  'word': '12</w>',\n  'start': 10,\n  'end': 12},\n {'entity': 'I_DTM',\n  'score': 0.9988531,\n  'index': 5,\n  'word': '_</w>',\n  'start': 13,\n  'end': 14},\n {'entity': 'E_DTM',\n  'score': 0.8283873,\n  'index': 6,\n  'word': 'มีนาคม</w>',\n  'start': 15,\n  'end': 21},\n {'entity': 'E_DTM',\n  'score': 0.5152181,\n  'index': 7,\n  'word': 'นี้</w>',\n  'start': 22,\n  'end': 25},\n {'entity': 'B_LOC',\n  'score': 0.97150415,\n  'index': 13,\n  'word': 'วัดพระแก้ว</w>',\n  'start': 45,\n  'end': 55},\n {'entity': 'B_LOC',\n  'score': 0.93494374,\n  'index': 16,\n  'word': 'กรุงเทพ</w>',\n  'start': 62,\n  'end': 69}]"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import PreTrainedTokenizerFast\ntokenizer = PreTrainedTokenizerFast.from_pretrained(\"new5558/HoogBERTa\")","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:51:53.440507Z","iopub.execute_input":"2023-04-05T18:51:53.440998Z","iopub.status.idle":"2023-04-05T18:51:53.737755Z","shell.execute_reply.started":"2023-04-05T18:51:53.440960Z","shell.execute_reply":"2023-04-05T18:51:53.736758Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--new5558--HoogBERTa/snapshots/0fadd23185e3ba91012653a71962c280fe97fa67/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--new5558--HoogBERTa/snapshots/0fadd23185e3ba91012653a71962c280fe97fa67/special_tokens_map.json\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--new5558--HoogBERTa/snapshots/0fadd23185e3ba91012653a71962c280fe97fa67/tokenizer_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.save_pretrained('saved_tokenizer')","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:51:53.739204Z","iopub.execute_input":"2023-04-05T18:51:53.740306Z","iopub.status.idle":"2023-04-05T18:51:53.826163Z","shell.execute_reply.started":"2023-04-05T18:51:53.740274Z","shell.execute_reply":"2023-04-05T18:51:53.825250Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"tokenizer config file saved in saved_tokenizer/tokenizer_config.json\nSpecial tokens file saved in saved_tokenizer/special_tokens_map.json\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"('saved_tokenizer/tokenizer_config.json',\n 'saved_tokenizer/special_tokens_map.json',\n 'saved_tokenizer/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer_loaded = PreTrainedTokenizerFast(tokenizer_file = \"saved_tokenizer/tokenizer.json\")","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:51:53.827584Z","iopub.execute_input":"2023-04-05T18:51:53.828847Z","iopub.status.idle":"2023-04-05T18:51:53.959465Z","shell.execute_reply.started":"2023-04-05T18:51:53.828808Z","shell.execute_reply":"2023-04-05T18:51:53.958236Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# huggingface_model_classification.push_to_hub()\n# tokenizer_loaded.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2023-04-05T18:51:53.961229Z","iopub.execute_input":"2023-04-05T18:51:53.962701Z","iopub.status.idle":"2023-04-05T18:51:53.971119Z","shell.execute_reply.started":"2023-04-05T18:51:53.962654Z","shell.execute_reply":"2023-04-05T18:51:53.969872Z"},"trusted":true},"execution_count":22,"outputs":[]}]}